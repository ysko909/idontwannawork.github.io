<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Codellama on 頑張らないために頑張る</title>
    <link>https://ysko909.github.io/tags/codellama/</link>
    <description>Recent content in Codellama on 頑張らないために頑張る</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>© Copyright ysko</copyright>
    <lastBuildDate>Fri, 13 Oct 2023 22:57:00 +0900</lastBuildDate>
    
	<atom:link href="https://ysko909.github.io/tags/codellama/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Google ColabでOpen InterpreterをCode Llamaで動かす</title>
      <link>https://ysko909.github.io/posts/use-open-interpreter-with-code-llama-on-colab/</link>
      <pubDate>Fri, 13 Oct 2023 22:57:00 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/use-open-interpreter-with-code-llama-on-colab/</guid>
      <description>概要 相変わらず話題のOpen Interpreter（以下、OI）を、今回はGoogle Colab上で動かしてみようというお話です。相変わらず、ローカルやColab上の限定された環境での動作を想定しているので、Code Llama（以下、Llama）を利用します。
以前の記事では、OIをは手元にあったショボショボPCを使ってローカルで動作させることをやってみました。このPCは、CPUはともかくロクなGPUも積んでないようなPCなので、OIを動作させたところでその真価を発揮できたかと言われると甚だ疑問です。そこで、「GPUをもうちょっと盛大に使ってOIを動作させたいー」という欲求を満たすため、Google Colaboratory（以下、Colab）を使います。
Google Colaboratoryとは 最初にGoogle Colaboratory（通称：Colab）について触れておきます。Colabは、Googleが提供するクラウドベースのPython開発環境です。
 Colab は、学生からデータ サイエンティスト、AI リサーチャーまで、皆さんの作業を効率化します。
  ブラウザ上でJupyter Notebookを記述し実行できる 環境構築が不要 GPUに料金なしでアクセス可能 簡単に共有可能  ColabではJupyter notebookの形式で基本的に記述するため、Pythonの理解がある人はすぐ使いこなせるでしょう。notebookは、実行可能コードやMarkdownによるテキストを、1つのドキュメントでそれぞれ記述できます。Colabで作成したnotebookは、GoogleドライブにColabのドキュメントとして保存されます。また、作成したドキュメントはリンクなどにより簡単に共有が可能で、コメントの記入や編集をしてもらうことも可能です。とりあえず作ってみると、使用感がわかるかと思います。
環境 先述のとおり、今回はColabを利用します。そのため、OIを動作させる環境の準備は、「Googleアカウントを取得してColabにログインする」で終わりです。あとは必要はライブラリやパッケージをインストールし、実行するだけです。この簡単さがColabの最大の利点ですね。
なお、Colabには無料プランと有料プランで使えるリソースに差があります。また、無料プランだとGPUが割り当てられるかは結構運任せで、だいたい割り当ててもらえません。CPUオンリーでも動作は可能ですが、それはローカルPCでもできることなので、今回はしっかりGPUを使いたいわけです。とは言え、先述のとおり無料プランだとGPUが割り当てられるかは確定しませんし、だいたいダメです。運に頼らずGPUを使いたいので、ここは素直に課金します。おかねのちからってすげー！
まぁ、課金するっていっても「Pro」です。さすがに「Pro＋」はオーバースペックじゃね・・・？いや、石油王とかならワンチャン？
ちなみにGPUが割り当てられると、上記のようにどんなGPUが割り当てられているか、CPUなどのリソースがどの程度利用されているかが小さく表示されます。ここではV100というGPUが割り当てられています。
OIを実行する方法 OIのインストール これは簡単です。pip installするだけですから。
!pip install open-interpreter 上記のコードを入力して実行します。少し待てば実行が終わり、ライブラリがインストールされたはずです。
OIの実行・・・ができない OIをインストールしたので、早速実行するぜ！・・・と行きたいところなんですが、残念。使えません。
実は、ここまで言っておいてなんですが、Colab上でCode Llamaで動かそうとしても動作しません。というか、そもそもOIの起動ができません。それはLlamaを実行するのに必要な--localオプションを付与した際の環境設定で、キー入力を受け付けずパラメータを指定できないからです。
!interpreter --local [?] Parameter count (smaller is faster, larger is more capable): 7B &amp;gt; 7B 13B 34B OIをLlamaで動作させる場合、上記のようにLlamaのオプション指定するよう表示され、任意の選択肢1つを指定します。するのですが、その際はカーソルキーで選択してから改行キーを押す必要があります。単純にColab上で!interpreter --localを実行してしまうと、Llamaのオプション設定をしたくともColabがキー入力を受け付けてくれません。そのため、Llamaのオプション指定ができない＝先に進まない＝Colab上でLlamaによるOIが実行できないとなるわけです。
また、OI公式を確認したところ、Open InterpreterのCode Llama利用については「Colabでの実行をサポートしていない」と書いてあります。
ターミナルの導入 とはいえ、ここで引き下がったらエンジニアの名折れです（暴言）。サポートされてないってだけで、完全にダメってことじゃないでしょうからどうにかなるはずです。なんとかなれー！
じゃあ、キー入力を受け付けてくれるにはどうするか？が問題になります。ここではColab上で動作するターミナルを導入します。
!pip install colab-xterm %load_ext colabxterm %xterm 最後の%xtermを実行すると、Colab上にターミナルが表示されます。ターミナルが表示されたら、改めてOIを起動します。</description>
    </item>
    
    <item>
      <title>Code LlamaによるOpen InterpreterをDockerコンテナ内で使う</title>
      <link>https://ysko909.github.io/posts/use-open-interpreter-with-code-llama-in-docker-container/</link>
      <pubDate>Tue, 03 Oct 2023 16:03:44 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/use-open-interpreter-with-code-llama-in-docker-container/</guid>
      <description>概要 今回は、話題のOpen InterpreterをCode Llamaを使ってローカル環境で使ってみようという話です。
Open Interpreterとは Open Interpreter（以下、OI）は、自然言語による対話からPythonやTypeScriptなどのプログラミングコードを生成し、実行する環境も整備してくれるオープンソースのソフトウェアです。OpenAIのChatGPTや、MetaのCode LlamaのLLMに基づいて動作します。
 An open-source, locally running implementation of OpenAI&amp;rsquo;s Code Interpreter.
 ここまで聞くと、ChatGPTの「Advanced Data Analysis」とよく似てるなーと思いますが、それのオープンソース版だと思ってもらえればいいかもしれません。ただ、このソフトはAdvanced Data Analysisと違ってローカル環境で動くことです。ChatGPTのAPIを利用することでGPT-3.5などを使って生成する場合、当たり前ですがインターネット環境は必須です。その点、Code Llamaを利用した場合は、完全にローカル環境で動作するため、ネット接続がない環境でも動作させることが可能です。
そのため、OIとCode Llamaの組み合わせを使えば、ブラウザの制御やファイル操作などのさまざまな作業を、チャット形式で生成し実行できます。しかもローカル環境で、です。
今回は、「ローカルで動かす」という部分にフォーカスしてOIを利用してみようと思います。さて、そもそもちゃんと動作してくれるんでしょうか。
環境 環境は手近にあった、ロクなGPUのないショボショボPCを利用しました。この時点で大分「ちゃんと動作する感」がしませんが、まぁそのなんだ、こまけぇこたぁいいんだよ！
 CPU: Intel&amp;reg; Core&amp;trade; i9-11900 @ 2.50GHz 2.50 GHz RAM: 16.0 GB OS: Windows 10上のWSL Ubuntu20.04  WSLのUbuntu上にDockerコンテナでOI用の環境を構築して、そのコンテナ内でOIをいじってみます。これでOIが何らかのライブラリをインストールしたり、あるいはファイルを削除するなどしても、コンテナ内に限定された出来事になります。
Dockerによる環境構築 入力したプロンプトによっては、環境構築のためにOIが勝手に各種のライブラリをインストールするらしいです。そのため、Dockerコンテナを構築してほかの環境を汚さないよう対処します。
Dockerfileの内容 とりあえずOIが動く程度の準備をざっくりします。
FROMpython:3.10-bookwormRUN apt-get update &amp;amp;&amp;amp; \  apt-get upgrade -y &amp;amp;&amp;amp; \  rm -rf /var/lib/apt/lists/*RUN pip install open-interpreter==0.</description>
    </item>
    
  </channel>
</rss>