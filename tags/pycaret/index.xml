<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pycaret on 頑張らないために頑張る</title>
    <link>https://ysko909.github.io/tags/pycaret/</link>
    <description>Recent content in Pycaret on 頑張らないために頑張る</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>© Copyright ysko</copyright>
    <lastBuildDate>Sat, 16 Oct 2021 14:45:04 +0900</lastBuildDate>
    
	<atom:link href="https://ysko909.github.io/tags/pycaret/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PyCaretをIrisやBostonで動かしてみる</title>
      <link>https://ysko909.github.io/posts/fundamentals-of-pycaret/</link>
      <pubDate>Sat, 16 Oct 2021 14:45:04 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/fundamentals-of-pycaret/</guid>
      <description>概要 PyCaretとは、AutoMLをサポートしたPythonの機械学習ライブラリです。AutoMLはというと、ある程度定型化されているような機械学習モデルの作成作業を自動化する仕組みのことです。
PyCaret自体は、scikit-learnやOptuna、Hyperoptなどの機械学習ライブラリのラッパー的な位置づけ。もちろん、LightGBMやCatboostにようなアルゴリズムもばっちりサポート。
scikit-learnで実装すると何行も書かなければならないようなモデルの学習ロジックを数行で実装できたり、本来matplotlibやseabornなどを使って描画するようなグラフもPyCaretでは1行で描画できたりと、とにかく機械学習における作業サイクルを簡素化して生産性向上に全振りしてる印象。しかも、PyCaret自体を実行するときも煩雑なコードを書く必要はまったくないのがすごい。
ここでは機械学習の代表的なデータを使って、PyCaretの使い方を見てみます。
PyCaretをIrisデータで使ってみる クラス分類では、毎度おなじみIris（アヤメ）のデータを使って、ざっくりPyCaretを実行してみます。
インストールは、こちらも毎度おなじみpipを使います。
pip install pycaret PyCaretのインストールは上記を実行しておきましょう。
なお、上記のコマンドでインストールされるPyCaretはSlim Versionであるため、一部の依存関係にあるライブラリをスキップしているようです。顕著に影響が出てくるケースがinterpert_model()あたりを実行する場合で、「XXXのライブラリがインストールされてないぞー」みたいなエラーが出てきたりますが、まぁ今回は気にしないでいきます。
pip install pycaret[full] 上記のコマンドを実行することで、PyCaretのFull Versionがインストールされます。
import pandas as pd import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from pycaret.classification import * まずはライブラリ読み込み。
ここではscikit-learnのload_iris()を使うのでimportしておきます。今回はクラス分類モデルの作成なのでpycaret.classificationを参照します。回帰モデルを作成する場合は別なクラスが用意されているのでそちらを参照します。
ちなみに後で気付いたのですが、PyCaretは代表的なデータを自分自身で提供している（ここでPyCaretが提供しているデータを確認できる）ので、わざわざscikit-learnをインポートする必要はなかったんだよなぁ・・・。
iris = load_iris() x = pd.DataFrame(data=iris.data, columns=iris.feature_names) y = pd.Series(data=iris.target, name=&amp;#39;Species&amp;#39;) 次にirisのデータをロードします。
train_X, test_X, train_y, test_y = train_test_split(x, y) train = pd.concat([train_X, train_y], axis=1) test = pd.concat([test_X, test_y], axis=1) 学習に使うトレーニングデータと、モデルを作ったあとの検証に使うテストデータを分けておきます。なお、ここでは説明変数と目的変数を1つのデータセットとして結合していますが、これは後で実行するPyCaretのセットアップで利用するため。</description>
    </item>
    
    <item>
      <title>PyCaretでSHAPを使った`interpret_model()`を実行するとエラーになる</title>
      <link>https://ysko909.github.io/posts/get-error-when-use-interpret-model-with-pycaret-and-shap/</link>
      <pubDate>Wed, 01 Sep 2021 23:51:17 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/get-error-when-use-interpret-model-with-pycaret-and-shap/</guid>
      <description> 前提 Pythonで機械学習をする際に有用なライブラリPyCaretの1機能であるinterpret_mode()を使うと、SHAPを利用したモデルの解釈をPyCaretから実行できるようになります。
pip install pycaret pip install shap この2行（正確にはinterpretも必要なプロットがあったりするけど）だけで、基本的にはPyCaretからSHAPが使えるようになるので非常に便利です。とくにNotebook上でPyCaretだけimportすればよく、import shapすら必要ありません。あとはinterpret_mode()と記述して、create_model()しておいたモデルについてプロットするだけです。
本来はそれだけのはずなのですが。
ええ、エラーになってしまったのですよ。
エラー内容 ImportError: numpy.core.multiarray failed to importRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd interpret_mode()を実行したところ、上記のようなエラーが発生しました。importの実行でエラーになってしまい、これから先に進みません。
場合によっては、importは実行できるもののinterpret_model()を実行した箇所で似たようなエラーになるケースにも遭遇しました（ただし、こちらについては後述）。
さらに解せないのは、同じコードでつい数日前までは実行できていたことです。数日前にinterpret_model()を正常に使っていたコンテナからDockerfileを持ってきて、それを元に新しくコンテナをビルドした環境で同じコードを実行したらエラーになる、という謎なシチュエーション。エラーメッセージを見るとNumPyが犯人っぽいけど・・・？
原因 Numbaのバージョンアップのせい。
実はNumbaの更新履歴を見てみると、2021年8月21日にバージョン0.54がリリースされていることがわかります。これが直接の原因です。
PyCaretは、NumPyなら後述のようにバージョン指定していますが、Numbaについてはとくにバージョン指定がありません。そのため、PyCaretをインストールするとNumbaは最新版がインストールされます。現時点（2021年9月1日）では0.54がインストールされます。ところが、このバージョンでinterpret_model()を実行すると、さっきのようなエラーが表示されてしまい、処理が異常終了してしまいます。
「数日前までは動いてた」ってのもコレが原因。たまたま8月21日以前にPyCaretの環境をDockerコンテナにて構築した際は、Numbaがバージョン0.53.1でインストールされたため問題なく動作していました。ところが、8月21日以降にPyCaretの環境を構築するとNumbaは最新バージョンをインストールしてしまいます。そのため、interpret_model()が正常に動作していた環境とは厳密には異なる環境になっていたわけです。
なお、エラーメッセージからしてNumPyが悪いように見えますが、これは完全に濡れ衣です。PyCaretはNumPyのバージョン1.19.5を指定している（以上でも以下でもなく==で指定）ため、少なくともフツーにpipするならこれ以外のバージョンがインストールされることはありません。
回避策 インストールするNumbaのバージョンを指定すればいいわけです。単純ですね。
numba==0.53.1 自分は、上記のように0.53.1をインストール対象のバージョンとして指定しています。コンテナであれば、上記のようにバージョン指定したうえでコンテナをリビルドすれば問題なくinterpret_model()が実行できるようになるはずです。
（参考）別解 基本的には上記のようにインストールするNumbaのバージョンを指定すれば問題ないはずですが、そんなことをしなくてもエラーになったセルを再度実行することで、何事もなかったかのように処理が通るケースも確認しました。なんでやねん。とくにimport部分ではなくinterpret_model()のセルでエラーを吐いているケースでは、単純に再実行するだけで処置が通りました。なんでやねん。理由は謎です。
なお、import部分でエラーになってしまうケースでは、この単純に再実行する方法は使えない（同じエラーが出続ける）ので、Numbaのバージョン指定を行うべきです。
いずれにせよ、なぜimport部分でコケるケースとコケないケースが存在するのかは謎です。とはいえ、エラーになる条件がいまいちはっきりせず、仮に再実行で処理の継続は可能とは言っても毎回エラーの度に再実行するのは手間なので、基本的には前述の通りNumbaのバージョン指定することがベターだと思います。
参考  Numba Numba history  </description>
    </item>
    
  </channel>
</rss>